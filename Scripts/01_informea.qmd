---
title: "01_informea"
format: html
---

This script downloads data from the InforMEA website

## Get treaties list from API

```{r}
library(httr)
library(jsonlite)
library(dplyr)
library(tibble)
library(purrr)
```

```{r}
treaties_url <- "https://odata.informea.org/informea.svc/Treaties"

resp <- GET(treaties_url, add_headers(Accept = "application/json"))
flat <- fromJSON(content(resp, as = "text", encoding = "UTF-8"), flatten = TRUE)


mea <- as_tibble(flat$d$results)
```

## Scrape parties and dates from web

```{r}
library(reticulate)

# Setup chunk for Quarto / R Markdown
knitr::opts_knit$set(root.dir = "C:/Users/aathens/OneDrive - United Nations/Documentos/CEPALSTAT Data Process/cepalstat-data-upload")

# Force use of known working path
use_python("C:/Users/aathens/AppData/Local/anaconda3/python.exe", required = TRUE)

# Double check config
py_config()
```

Set up Playwright session

```{python}
from playwright.sync_api import sync_playwright
p = sync_playwright().start()
browser = p.chromium.launch(headless=False)
context = browser.new_context(accept_downloads=True)
page = context.new_page()
```

Navigate to InforMEA website and scrape parties

```{python}
import pandas as pd
import time

mea_full = r.mea
all_parties = []

# while testing, use a shorter version of df
mea = mea_full.head(6)

# row_id, row = next(mea.iterrows()) # for testing index = 0

for row_id, row in mea.iterrows():
  url = row["url"]
  treaty_id = row["id"]
  
  try:
      page.goto(url + "/treaty-parties")
      page.wait_for_load_state("networkidle")
      page.wait_for_selector("table", timeout=10000)
      
      ## Get headers
      # Scrape headers dynamically from the first table (active party members only, excluding non-parties)
      headers = [th.inner_text() for th in page.query_selector_all("table:first-of-type thead th")] 
      
      # De-duplicate headers by appending a counter to any duplicates
      from collections import Counter
      counts = Counter()
      unique_headers = []
      for h in headers:
        counts[h] += 1
        if counts[h] > 1:
            unique_headers.append(f"{h}_{counts[h]}")
        else:
            unique_headers.append(h)
      
      ## Get row data
      rows = page.query_selector_all("table:first-of-type tbody tr") # extract all rows from the first table
      data = []
      for row in rows:
          cells = row.query_selector_all("td") # extract references to html nodes
          data.append([cell.inner_text() for cell in cells]) # extract node values (table text)
      
      # Keep only rows that match the number of headers
      data = [row for row in data if len(row) == len(headers)]
      # note this will drop non-parties and declarations/footnotes
      
      parties = pd.DataFrame(data, columns=unique_headers)
      
      # Tag each row with its source treaty URL
      parties["treaty_id"] = treaty_id
      all_parties.append(parties)
      
      print(f"Party data scraped for {row_id + 1}/{len(mea)}")
      time.sleep(2)
      
  except Exception as e:
      print(f"Failed for {url}: {e}")
  

all_parties_df = pd.concat(all_parties, ignore_index=True)
```



```{python}
import pandas as pd
mea = r.mea

# mea.columns.tolist()
url = mea[["url"]].iloc[1].item()
```


```{python}
page.goto(url)

page.click("treaty-parties")
```

```{python}
page.wait_for_selector("table")

# Extract all rows from the table
rows = page.query_selector_all("table tbody tr")

data = []
for row in rows:
    cells = row.query_selector_all("td") # extract references to html nodes
    data.append([cell.inner_text() for cell in cells]) # extract node values

# Keep only entries with 4 or 5 columns (drop declaration text)    
data = [row for row in data if len(row) == 5]
# include 4 to keep non-parties (Haiti, US)
    
# Pull column headers
headers = ['Party', 'Signature', 'Ratification', 'Status', 'Additional information']

parties = pd.DataFrame(data, columns=headers)
```

Intermediate checks for what's being pulled from tables
```{python}
# for row in data:
#     print(row)
```

Read back into R
```{r}
parties <- as_tibble(py$all_parties_df)
```

```{r}
treaty_ids <- parties %>% distinct(treaty_id)

mea %>% 
  left_join(treaty_ids %>% mutate(pres = 1), by = c("id" = "treaty_id")) %>% 
  select(id, url, pres)
```

Scrape treaty metadata
```{python}
import pandas as pd
import time

mea_full = r.mea
all_treaties = []

# while testing, use a shorter version of df
mea = mea_full.head(6)

# row_id, row = next(mea.iterrows()) # for testing index = 0

for row_id, row in mea.iterrows():
  url = row["url"]
  treaty_id = row["id"]
  
  try:
      page.goto(url)
      page.wait_for_load_state("networkidle")
      
      # Get fields from main page
      fields = {}
      field_divs = page.query_selector_all(".field.d-flex")
      
      for div in field_divs:
          label = div.query_selector(".field__label")
          value = div.query_selector(".field__item")
          if label and value:
              key = label.inner_text().replace(":", "").strip()
              fields[key] = value.inner_text().strip()
      
      # Add summary at top of page
      fields["Summary"] = page.inner_text(".treaty__body").strip()
      
      treaty_info = pd.DataFrame([fields])
      treaty_info["treaty_id"] = treaty_id
      all_treaties.append(treaty_info)
      
      print(f"Treaty data scraped for {row_id + 1}/{len(mea)}")
      time.sleep(2)
      
  except Exception as e:
      print(f"Failed for {url}: {e}")
  

all_treaties_df = pd.concat(all_treaties, ignore_index=True)
```

Read back into R
```{r}
treaties <- as_tibble(py$all_treaties_df)
```

```{r}
treaty_ids <- treaties %>% distinct(treaty_id)

mea %>% 
  left_join(treaty_ids %>% mutate(pres = 1), by = c("id" = "treaty_id")) %>% 
  select(id, url, pres)
```
