---
title: "01_informea"
format: html
---

This script downloads environmental treaty data from the InforMEA website

## Get treaties list from API

```{r}
library(httr)
library(jsonlite)
library(dplyr)
library(tibble)
library(purrr)
```

```{r}
treaties_url <- "https://odata.informea.org/informea.svc/Treaties"

resp <- GET(treaties_url, add_headers(Accept = "application/json"))
flat <- fromJSON(content(resp, as = "text", encoding = "UTF-8"), flatten = TRUE)


mea_full <- as_tibble(flat$d$results)
mea <- mea_full
```

Filter on ECLAC-relevant treaties
```{r}
# get names of treaties currently in CEPALSTAT
# alberto <- c("ramsar", "heritage", "cites", "cms", "law of the sea", "vienna", "montreal", "basel", "biological diversity", "climate change", "desertification", 
#              "kyoto", "rotterdam", "cartagena", "stockholm", "paris-unfccc", "minimata", "escazu")

# make any slight modifications for matches to work better
# shorten to paris, fix minamata spelling, add cartagena-conv
alberto_mod <- c("ramsar", "heritage", "cites", "cms", "law of the sea", "vienna", "montreal", "basel", "biological diversity", "climate change", "desertification", 
             "kyoto", "rotterdam", "cartagena", "stockholm", "paris", "minamata", "escazu", "cartagena-conv")

# add cepalstat keyword to df
mea %<>%
  mutate(
    matched_keyword = coalesce(
      str_extract(str_to_lower(officialNameEnglish), str_c(alberto_mod, collapse = "|")),
      if_else(id %in% alberto_mod, id, NA_character_)
    )
  ) %>% 
  relocate(matched_keyword, .after = id)

# check if any keywords didn't match to mea
# alberto_mod[which(!alberto_mod %in% unique(mea$matched_keyword))]

# check for duplicates
# mea %>% group_by(matched_keyword) %>% count() %>% filter(n>1)

# fix duplicates
mea %<>% 
  mutate(matched_keyword = ifelse(id %in% c("basel-protocol-liability", "nagoya", "barc-spa", "NKL", "bbnj"), NA_character_, matched_keyword))

# manually check whether the other treaties are relevant to the region
# mea %>% filter(is.na(matched_keyword)) %>% View()

# mea %<>% 
#   mutate(matched_keyword = case_when(
#     id == "cartagena-conv" ~ "cartagena-conv", # for wider caribbean region
#     TRUE ~ matched_keyword
#   ))

# drop all other treaties, not regionally important
mea %<>% 
  filter(!is.na(matched_keyword)) %>% 
  rename(keyword = matched_keyword)
```


## Setup
```{r}
library(reticulate)

# Setup chunk for Quarto / R Markdown
knitr::opts_knit$set(root.dir = "C:/Users/aathens/OneDrive - United Nations/Documentos/CEPALSTAT Data Process/cepalstat-data-upload")

# Force use of known working path
use_python("C:/Users/aathens/AppData/Local/anaconda3/python.exe", required = TRUE)

# Double check config
py_config()
```

Set up Playwright session
```{python}
from playwright.sync_api import sync_playwright
p = sync_playwright().start()
browser = p.chromium.launch(headless=False)
context = browser.new_context(accept_downloads=True)
page = context.new_page()
```

## Scrape party data from web
```{python}
import pandas as pd
import time

mea = r.mea
all_parties = []

# while testing, use a shorter version of df
# mea = mea_full.head(6)

# row_id, row = next(mea.iterrows()) # for testing index = 0

for row_id, row in mea.iterrows():
  url = row["url"]
  treaty_id = row["id"]
  
  try:
      page.goto(url + "/treaty-parties")
      page.wait_for_load_state("networkidle")
      page.wait_for_selector("table", timeout=20000)
      
      ## Get headers
      # Scrape headers dynamically from the first table (active party members only, excluding non-parties)
      headers = [th.inner_text() for th in page.query_selector_all("table:first-of-type thead th")] 
      
      # De-duplicate headers by appending a counter to any duplicates
      from collections import Counter
      counts = Counter()
      unique_headers = []
      for h in headers:
        counts[h] += 1
        if counts[h] > 1:
            unique_headers.append(f"{h}_{counts[h]}")
        else:
            unique_headers.append(h)
      
      ## Get row data
      rows = page.query_selector_all("table:first-of-type tbody tr") # extract all rows from the first table
      data = []
      for row in rows:
          cells = row.query_selector_all("td") # extract references to html nodes
          data.append([cell.inner_text() for cell in cells]) # extract node values (table text)
      
      # Keep only rows that match the number of headers
      data = [row for row in data if len(row) == len(headers)]
      # note this will drop non-parties and declarations/footnotes
      
      parties = pd.DataFrame(data, columns=unique_headers)
      
      # Tag each row with its source treaty URL
      parties["treaty_id"] = treaty_id
      all_parties.append(parties)
      
      print(f"Party data scraped for {row_id + 1}/{len(mea)} treaties")
      time.sleep(2)
      
  except Exception as e:
      print(f"Failed for {url}: {e}")
  

all_parties_df = pd.concat(all_parties, ignore_index=True)
```

Check for missing treaties
```{python}
all_parties_df["treaty_id"].unique().tolist()

# check which treaty parties weren't found
set(r.mea["id"].tolist()) - set(all_parties_df["treaty_id"].unique().tolist())
# ok to be missing: heritage (no parties)
```

## Scrape treaty metadata from web
```{python}
import pandas as pd
import time

mea = r.mea
all_treaties = []

# while testing, use a shorter version of df
# mea = mea_full.head(6)

# row_id, row = next(mea.iterrows()) # for testing index = 0

for row_id, row in mea.iterrows():
  url = row["url"]
  treaty_id = row["id"]
  
  try:
      page.goto(url)
      page.wait_for_load_state("networkidle")
      
      # Get fields from main page
      fields = {}
      field_divs = page.query_selector_all(".field.d-flex")
      
      for div in field_divs:
          label = div.query_selector(".field__label")
          value = div.query_selector(".field__item")
          if label and value:
              key = label.inner_text().replace(":", "").strip()
              fields[key] = value.inner_text().strip()
      
      # Add summary at top of page
      fields["Summary"] = page.inner_text(".treaty__body").strip()
      
      treaty_info = pd.DataFrame([fields])
      treaty_info["treaty_id"] = treaty_id
      all_treaties.append(treaty_info)
      
      print(f"Treaty data scraped for {row_id + 1}/{len(mea)} treaties")
      time.sleep(2)
      
  except Exception as e:
      print(f"Failed for {url}: {e}")
  

all_treaties_df = pd.concat(all_treaties, ignore_index=True)
```

Check for missing treaties
```{python}
# all_treaties_df["treaty_id"].unique().tolist()

# check which treaty parties weren't found
set(r.mea["id"].tolist()) - set(all_treaties_df["treaty_id"].unique().tolist())
# ok to be missing: heritage (no parties)
```

## Clean party data
Read back into R
```{r}
parties <- as_tibble(py$all_parties_df)
```

```{r}
treaty_ids <- parties %>% distinct(treaty_id)

mea %>% 
  left_join(treaty_ids %>% mutate(pres = 1), by = c("id" = "treaty_id")) %>% 
  select(id, url, pres)
```

## Clean treaty data
Read back into R
```{r}
treaties <- as_tibble(py$all_treaties_df)
```

```{r}
treaty_ids <- treaties %>% distinct(treaty_id)

mea %>% 
  left_join(treaty_ids %>% mutate(pres = 1), by = c("id" = "treaty_id")) %>% 
  select(id, url, pres)
```

## Export flat files
